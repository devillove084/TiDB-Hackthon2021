# TiDB-Hackthon2021
## 设计题目：BPF for storage

> 作者：devillove084
>
> 项目进展：完成idea梳理以及可行性分析，环境搭建完毕并且解决工具链的匹配等问题。

## 项目介绍

### 动机和背景

Kernel-bypass框架(如SPDK)以及用户态存储的处理方式可以降低内核开销，但通常需要定制，需要对应用程序进行更改，缺乏隔离性，在I/O使用率不高时会浪费大量等待时间，并且在计算存储中需要定制硬件。所以，本次设计希望使用一个支持标准OS的机制来降低高速存储设备的软件开销。当用于拦截I/O时，可以在应用中使用这些函数进行处理，以此来避免内核和用户空间的上下文切换和数据拷贝。

#### eBPF

Linux eBPF通常用于网路数据包处理和过滤，以及追踪函数执行路径。Linux的eBPF为应用提供了一种直接在内核中嵌入函数的接口，广泛用于在网络栈外针对特定应用进行内核侧扩展。在本次设计中，eBPF可以用于链式依赖的IO路径，消除内核存储栈传输以及与用户空间的数据转换造成的开销。例如在遍历一个基于磁盘的数据结构(B+树)，通过在内核中嵌入这些eBPF函数，就可以像kernel-bypass一样消除发起IO路径造成的开销，但是和kernel-bypass不同，这里不需要轮询避免浪费CPU clock。所以本次设计则用于通过绕过存储栈(文件系统、BIO等层)来提高存储的读写效率，同时在设计中会遇到诸挑战，如文件和块的映射关系，多进程共享存储块以及进程间的QoS等。



为了简化设计开销以及易于移植和使用，提出以下约束：

1. 在架构上必须支持标准的Linux文件系统，并尽量减少对应用的修改，同时应该兼具灵活性，且能够高效运行并绕过大量软件层；
2. 存储BPF需要了解应用的磁盘格式，并访问外部的应用状态或内核状态；
3. 保证BPF存储函数在允许应用间共享磁盘容量时不会违背文件系统的安全性；
4. 保证并发性。应用程序需要通过细粒度同步来支持并发访问，以此避免读写对高吞吐量的影响。

#### Xok Exokernel

> * Exokernel是由麻省理工学院开发的一种操作系统，旨在提供硬件资源的应用程序级管理。 外部内核体系结构旨在将资源保护与管理分开，以促进特定于应用程序的自定义。常规OS始终会对构建在其上的应用程序的性能，功能和范围产生影响。因为OS位于应用程序和物理硬件之间。 Exokernel试图通过**<u>*消除OS必须提供构建应用程序的抽象这一概念*</u>**来解决此问题，目的是构建一个小的内核将所有硬件抽象都移到称为libOS的不受信任的库中。 最后，Exokernel的主要目标是确保没有强制内核级硬件抽象，这是与微内核和单核不同的原因。
> * XN是storage system，本质上是一个block layer，而libOS是上层的文件系统，用来实现inode以及metadata等抽象，进而实现ext4，FAT等文件系统。而每一个libOS可以有多个libFS。
> * 核心概念：
>   * 所有的access control都是统一的，这体现在了磁盘和内存的控制上。
>   * 把硬件与抽象层绑定；
>   * downloaded code：把用户态的代码引入(download)到内核态运行并且，文件可能需要对其验证时间进行更新;



UDF是XN的基石(用于Xok Exokernel)，通过从用户进程下载到内核的代码来支持互不信任的"libfs"扩展存储。在XN中，这些不可信的功能被解释为***<u>==让内核按照用户定义的方式来理解文件系统元数据==</u>***。这种方式不会影响应用和内核设计，且满足了文件系统布局的灵活性目标。这里的设计就需要一个类似的机制来允许用户让内核理解定义的数据布局，但实现的目标不同：在设计上允许应用使用自定义的兼容BPF的数据结构以及传输和过滤磁盘数据的功能，且能够与Linux现有的接口和文件系统一起运作，通过大幅削减每个I/O执行所需要的内核代码量来提高IOPS。



## 项目设计

设计构建一个可以提供比BPF更高层的接口库，以及新的BPF hook，且尽可能早于存储I/O completion路径(类似XDP)，用于加速访问和操作特定数据结构的BPF函数，如B树和LSM。在内核中，在每个block I/O结束后，存储驱动中断处理器可能会触发这些BPF函数(目的是定义构成应用数据结构的存储块布局)。通过给予这些函数访问块数据原始缓冲的功能，可以从存储的块中抽取文件偏移，并立即使用这些偏移发起I/O。它们还可以通过后续返回给应用的数据来过滤和汇总blocks。



1. 如何执行，编写BPF函数，会对应用发起I/O时使用的文件描述符打标签，传递到存储层时也会携带该标签，后续会在触发设备中断处理器的内核I/O completion路径上检查该标签。对于每个打标签的submission/completion请求，中断处理器hook会将读存储块传递到BPF函数中。当触发hook时，BPF函数可以执行一部分工作。例如，它可以从块中抽取文件偏移，编写逻辑回收submission描述符和I/O缓冲，将描述符重新定位到新的偏移并将其重新发布到存储设备提交队列；

2. 转换，在Linux中，存储(NVMe)驱动无法访问文件系统元数据，所以要向存储驱动层提供足够的信息，有效安全地将文件偏移映射到文件对应的相应物理块偏移，而不会限制文件系统重映射(选择)块的能力。为了简化设计和保证安全性，每个函数仅会使用1中提到到的文件的偏移量；

3. I/O粒度不匹配，当BIO层分割一个I/O时(如跨两个不连续的扩展)，会在不同的时间产生多个存储操作。期望尽量减少这类情况，执行BPF函数，并在内核开始下一次"hop"时重启I/O链，这样可以避免额外的内核复杂性。而如果应用需要生成更多的I/O来响应一个I/O completion，我们可以将该completion传播到BIO层，在该层可以分配并将多个I/O提交到存储层，避免应用参与处理；

4. Cache，BPF函数不会直接与缓存进行交互，但可以将特定的对象返回给应用(不一定是分页)，这样应用就可以使用自定义的缓存；

5. 并发性和公平性，从文件系统发起的写可能只会反应在缓存(Page Cache)中，而在BPF中不可见，可以通过锁来解决这个问题，但从内核内部来管理应用程序级别的锁代价很大，所以这里需要精心设计细粒度锁的数据结构。同时为了避免读/写冲突，计划使用非原地更新的数据结构，比如LSM SST，B树。并且由于NVMe设备默认的Linux块层调度器是noop，如果在需要公平性约束时，需要仲裁。

## 具体设计

1. 使用Rust for Linux构建Rust Module版本的Linux内核，以及尝试编写Rust 版本的libbpf工具函数demo，并进行验证；
2. 构建特殊的BPF存储Trace函数，使用1中的定制内核进行hook并trace；
3. 尝试合并内核中的TiKV相关的I/O路径；
4. 尝试使用1构建的内核编写内核版本的并发数据结构，进一步构建数据结构存储内核中存储I/O路径以及锁和文件偏移；
5. (未完待续)。。



