diff --git a/Makefile b/Makefile
index 24a4c1b97bb0..d702769637fd 100644
--- a/Makefile
+++ b/Makefile
@@ -2,7 +2,7 @@
 VERSION = 5
 PATCHLEVEL = 8
 SUBLEVEL = 0
-EXTRAVERSION =
+EXTRAVERSION =-bpf-storage-nvme
 NAME = Kleptomaniac Octopus
 
 # *DOCUMENTATION*
diff --git a/arch/x86/entry/syscalls/syscall_64.tbl b/arch/x86/entry/syscalls/syscall_64.tbl
index 78847b32e137..f9a4b4b7bdda 100644
--- a/arch/x86/entry/syscalls/syscall_64.tbl
+++ b/arch/x86/entry/syscalls/syscall_64.tbl
@@ -360,6 +360,7 @@
 437	common	openat2			sys_openat2
 438	common	pidfd_getfd		sys_pidfd_getfd
 439	common	faccessat2		sys_faccessat2
+440	common	set_bpf_level		sys_set_bpf_level
 
 #
 # x32-specific system call numbers start at 512 to avoid cache impact
diff --git a/block/blk-core.c b/block/blk-core.c
index 03252af8c82c..41345936edcf 100644
--- a/block/blk-core.c
+++ b/block/blk-core.c
@@ -907,6 +907,9 @@ static inline int blk_partition_remap(struct bio *bio)
 		if (bio_check_eod(bio, part_nr_sects_read(p)))
 			goto out;
 		bio->bi_iter.bi_sector += p->start_sect;
+		if (bio->_bpf_level > 0) {
+			bio->_bpf_partition_start_sector = p->start_sect;
+		}
 		trace_block_bio_remap(bio->bi_disk->queue, bio, part_devt(p),
 				      bio->bi_iter.bi_sector - p->start_sect);
 	}
diff --git a/drivers/nvme/host/nvme.h b/drivers/nvme/host/nvme.h
index 09ffc3246f60..64d076067e3e 100644
--- a/drivers/nvme/host/nvme.h
+++ b/drivers/nvme/host/nvme.h
@@ -485,6 +485,10 @@ static inline void nvme_end_request(struct request *req, __le16 status,
 		union nvme_result result)
 {
 	struct nvme_request *rq = nvme_req(req);
+	if (req->_bpf_command) {
+		kfree(req->_bpf_command);
+		req->_bpf_command = NULL;
+	}
 
 	rq->status = le16_to_cpu(status) >> 1;
 	rq->result = result;
diff --git a/drivers/nvme/host/pci.c b/drivers/nvme/host/pci.c
index d4b1ff747123..ed1dbb010552 100644
--- a/drivers/nvme/host/pci.c
+++ b/drivers/nvme/host/pci.c
@@ -24,6 +24,8 @@
 #include <linux/io-64-nonatomic-lo-hi.h>
 #include <linux/sed-opal.h>
 #include <linux/pci-p2pdma.h>
+#include <linux/bpf.h>
+#include <linux/filter.h>
 
 #include "trace.h"
 #include "nvme.h"
@@ -469,14 +471,15 @@ static inline void nvme_write_sq_db(struct nvme_queue *nvmeq)
 static void nvme_submit_cmd(struct nvme_queue *nvmeq, struct nvme_command *cmd,
 			    bool write_sq)
 {
-	spin_lock(&nvmeq->sq_lock);
+	unsigned long flags;
+	spin_lock_irqsave(&nvmeq->sq_lock, flags);
 	memcpy(nvmeq->sq_cmds + (nvmeq->sq_tail << nvmeq->sqes),
 	       cmd, sizeof(*cmd));
 	if (++nvmeq->sq_tail == nvmeq->q_depth)
 		nvmeq->sq_tail = 0;
 	if (write_sq)
 		nvme_write_sq_db(nvmeq);
-	spin_unlock(&nvmeq->sq_lock);
+	spin_unlock_irqrestore(&nvmeq->sq_lock, flags);
 }
 
 static void nvme_commit_rqs(struct blk_mq_hw_ctx *hctx)
@@ -860,9 +863,23 @@ static blk_status_t nvme_queue_rq(struct blk_mq_hw_ctx *hctx,
 	struct nvme_dev *dev = nvmeq->dev;
 	struct request *req = bd->rq;
 	struct nvme_iod *iod = blk_mq_rq_to_pdu(req);
-	struct nvme_command cmnd;
+	struct nvme_command cmnd, *cmndp;
 	blk_status_t ret;
 
+	if (req->bio && req->bio->_bpf_level > 0) {
+		cmndp = kmalloc(sizeof(struct nvme_command), GFP_NOWAIT);
+		if (!cmndp) {
+			printk("nvme_queue_rq: failed to allocate struct nvme_command\n");
+			cmndp = &cmnd;
+			req->_bpf_command = NULL;
+		} else {
+			req->_bpf_command = cmndp;
+		}
+	} else {
+		cmndp = &cmnd;
+		req->_bpf_command = NULL;
+	}
+
 	iod->aborted = 0;
 	iod->npages = -1;
 	iod->nents = 0;
@@ -874,24 +891,24 @@ static blk_status_t nvme_queue_rq(struct blk_mq_hw_ctx *hctx,
 	if (unlikely(!test_bit(NVMEQ_ENABLED, &nvmeq->flags)))
 		return BLK_STS_IOERR;
 
-	ret = nvme_setup_cmd(ns, req, &cmnd);
+	ret = nvme_setup_cmd(ns, req, cmndp);
 	if (ret)
 		return ret;
 
 	if (blk_rq_nr_phys_segments(req)) {
-		ret = nvme_map_data(dev, req, &cmnd);
+		ret = nvme_map_data(dev, req, cmndp);
 		if (ret)
 			goto out_free_cmd;
 	}
 
 	if (blk_integrity_rq(req)) {
-		ret = nvme_map_metadata(dev, req, &cmnd);
+		ret = nvme_map_metadata(dev, req, cmndp);
 		if (ret)
 			goto out_unmap_data;
 	}
 
 	blk_mq_start_request(req);
-	nvme_submit_cmd(nvmeq, &cmnd, bd->last);
+	nvme_submit_cmd(nvmeq, cmndp, bd->last);
 	return BLK_STS_OK;
 out_unmap_data:
 	nvme_unmap_data(dev, req);
@@ -937,10 +954,16 @@ static inline struct blk_mq_tags *nvme_queue_tagset(struct nvme_queue *nvmeq)
 	return nvmeq->dev->tagset.tags[nvmeq->qid - 1];
 }
 
+extern struct bpf_prog __rcu *_bpf_prog;
+extern struct bpf_storage_kern _bpf_g_context;
+
 static inline void nvme_handle_cqe(struct nvme_queue *nvmeq, u16 idx)
 {
 	struct nvme_completion *cqe = &nvmeq->cqes[idx];
 	struct request *req;
+	long _index;
+	struct bpf_prog *_local_bpf_prog;
+	u32 _bpf_return;
 
 	if (unlikely(cqe->command_id >= nvmeq->q_depth)) {
 		dev_warn(nvmeq->dev->ctrl.device,
@@ -963,7 +986,31 @@ static inline void nvme_handle_cqe(struct nvme_queue *nvmeq, u16 idx)
 
 	req = blk_mq_tag_to_rq(nvme_queue_tagset(nvmeq), cqe->command_id);
 	trace_nvme_sq(req, cqe->sq_head, nvmeq->sq_tail);
-	nvme_end_request(req, cqe->status, cqe->result);
+
+	if (!req->bio || req->bio->_bpf_level == 0) {
+		nvme_end_request(req, cqe->status, cqe->result);
+	} else {
+		++req->bio->_bpf_count;
+		if (req->bio->_bpf_count < req->bio->_bpf_level) {
+			/* resubmit another request */
+			rcu_read_lock();
+			_local_bpf_prog = rcu_dereference(_bpf_prog);
+			if (_local_bpf_prog) {
+				/* run bpf program if present */
+				_bpf_return = BPF_PROG_RUN(_local_bpf_prog, &_bpf_g_context);
+			}
+			rcu_read_unlock();
+			_index = req->bio->bi_iter.bi_sector >> (12 - 9);
+			_index = (_index * 1103515245 + 12345) % (1 << 23);  /* randomly choose the next offset */
+			req->bio->bi_iter.bi_sector = _index << (12 - 9);
+			req->__sector = req->bio->bi_iter.bi_sector + req->bio->_bpf_partition_start_sector;
+			req->_bpf_command->rw.slba = cpu_to_le64(nvme_sect_to_lba(req->q->queuedata, blk_rq_pos(req)));
+			nvme_submit_cmd(nvmeq, req->_bpf_command, true);
+		} else {
+			/* complete this IO chain */
+			nvme_end_request(req, cqe->status, cqe->result);
+		}
+	}
 }
 
 static inline void nvme_update_cq_head(struct nvme_queue *nvmeq)
diff --git a/fs/block_dev.c b/fs/block_dev.c
index 0ae656e022fd..98ba2b352d81 100644
--- a/fs/block_dev.c
+++ b/fs/block_dev.c
@@ -231,6 +231,9 @@ __blkdev_direct_IO_simple(struct kiocb *iocb, struct iov_iter *iter,
 	bio.bi_end_io = blkdev_bio_end_io_simple;
 	bio.bi_ioprio = iocb->ki_ioprio;
 
+	bio._bpf_level = file->_bpf_level;
+	bio._bpf_partition_start_sector = 0;
+
 	ret = bio_iov_iter_get_pages(&bio, iter);
 	if (unlikely(ret))
 		goto out;
@@ -381,6 +384,8 @@ __blkdev_direct_IO(struct kiocb *iocb, struct iov_iter *iter, int nr_pages)
 		bio->bi_private = dio;
 		bio->bi_end_io = blkdev_bio_end_io;
 		bio->bi_ioprio = iocb->ki_ioprio;
+		bio->_bpf_level = file->_bpf_level;
+		bio->_bpf_partition_start_sector = 0;
 
 		ret = bio_iov_iter_get_pages(bio, iter);
 		if (unlikely(ret)) {
diff --git a/fs/ioctl.c b/fs/ioctl.c
index d69786d1dd91..9ccd5201331d 100644
--- a/fs/ioctl.c
+++ b/fs/ioctl.c
@@ -19,6 +19,8 @@
 #include <linux/falloc.h>
 #include <linux/sched/signal.h>
 #include <linux/fiemap.h>
+#include <linux/bpf.h>
+#include <linux/filter.h>
 
 #include "internal.h"
 
@@ -762,6 +764,57 @@ SYSCALL_DEFINE3(ioctl, unsigned int, fd, unsigned int, cmd, unsigned long, arg)
 	return ksys_ioctl(fd, cmd, arg);
 }
 
+SYSCALL_DEFINE2(set_bpf_level, int, fd, int, level)
+{
+	struct fd f = fdget_pos(fd);
+	long ret = -EBADF;
+
+	if (f.file) {
+		f.file->_bpf_level = level;
+		fdput_pos(f);
+		ret = 0;
+	} else {
+		printk("set_bpf_level: bad file descriptor\n");
+	}
+
+	return ret;
+}
+
+struct bpf_prog __rcu *_bpf_prog;
+EXPORT_SYMBOL(_bpf_prog);
+
+struct bpf_storage_kern _bpf_g_context;
+EXPORT_SYMBOL(_bpf_g_context);
+
+int _storage_bpf_prog_attach(const union bpf_attr *attr, struct bpf_prog *prog)
+{
+	rcu_assign_pointer(_bpf_prog, prog);
+	return 0;
+}
+
+int _storage_bpf_prog_detach(const union bpf_attr *attr)
+{
+	rcu_assign_pointer(_bpf_prog, NULL);
+	return 0;
+}
+
+const struct bpf_prog_ops storage_prog_ops = {};
+
+static const struct bpf_func_proto *
+storage_func_proto(enum bpf_func_id func_id, const struct bpf_prog *prog)
+{
+	return bpf_base_func_proto(func_id);
+}
+
+static bool storage_is_valid_access(int off, int size, enum bpf_access_type type, const struct bpf_prog *prog, struct bpf_insn_access_aux *info){
+	return true;
+}
+
+const struct bpf_verifier_ops storage_verifier_ops = {
+	.get_func_proto = storage_func_proto,
+	.is_valid_access = storage_is_valid_access,
+};
+
 #ifdef CONFIG_COMPAT
 /**
  * compat_ptr_ioctl - generic implementation of .compat_ioctl file operation
diff --git a/fs/iomap/direct-io.c b/fs/iomap/direct-io.c
index ec7b78e6feca..a6e53c8b311d 100644
--- a/fs/iomap/direct-io.c
+++ b/fs/iomap/direct-io.c
@@ -277,6 +277,9 @@ iomap_dio_bio_actor(struct inode *inode, loff_t pos, loff_t length,
 		bio->bi_private = dio;
 		bio->bi_end_io = iomap_dio_bio_end_io;
 
+		bio->_bpf_level = dio->iocb->ki_filp->_bpf_level;
+		bio->_bpf_partition_start_sector = 0;
+
 		ret = bio_iov_iter_get_pages(bio, dio->submit.iter);
 		if (unlikely(ret)) {
 			/*
diff --git a/include/linux/blk_types.h b/include/linux/blk_types.h
index ccb895f911b1..013d8aa2396e 100644
--- a/include/linux/blk_types.h
+++ b/include/linux/blk_types.h
@@ -211,6 +211,10 @@ struct bio {
 
 	struct bio_set		*bi_pool;
 
+	int _bpf_level;
+	int _bpf_count;
+	u64 _bpf_partition_start_sector;
+
 	/*
 	 * We can inline a number of vecs at the end of the bio, to avoid
 	 * double allocations for a small number of bio_vecs. This member
diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 57241417ff2f..c59a532c0376 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -27,6 +27,7 @@
 #include <linux/percpu-refcount.h>
 #include <linux/scatterlist.h>
 #include <linux/blkzoned.h>
+#include <linux/nvme.h>
 
 struct module;
 struct scsi_ioctl_command;
@@ -241,6 +242,8 @@ struct request {
 		u64 fifo_time;
 	};
 
+	struct nvme_command *_bpf_command;
+
 	/*
 	 * completion callback.
 	 */
diff --git a/include/linux/bpf_types.h b/include/linux/bpf_types.h
index a18ae82a298a..53e695703b57 100644
--- a/include/linux/bpf_types.h
+++ b/include/linux/bpf_types.h
@@ -76,6 +76,9 @@ BPF_PROG_TYPE(BPF_PROG_TYPE_LSM, lsm,
 #endif /* CONFIG_BPF_LSM */
 #endif
 
+BPF_PROG_TYPE(BPF_PROG_TYPE_STORAGE, storage,
+              struct bpf_storage, struct bpf_storage_kern)
+
 BPF_MAP_TYPE(BPF_MAP_TYPE_ARRAY, array_map_ops)
 BPF_MAP_TYPE(BPF_MAP_TYPE_PERCPU_ARRAY, percpu_array_map_ops)
 BPF_MAP_TYPE(BPF_MAP_TYPE_PROG_ARRAY, prog_array_map_ops)
diff --git a/include/linux/filter.h b/include/linux/filter.h
index 0b0144752d78..cb81a2458192 100644
--- a/include/linux/filter.h
+++ b/include/linux/filter.h
@@ -1278,4 +1278,8 @@ struct bpf_sockopt_kern {
 	s32		retval;
 };
 
+struct bpf_storage_kern {
+	char data[512];
+};
+
 #endif /* __LINUX_FILTER_H__ */
diff --git a/include/linux/fs.h b/include/linux/fs.h
index f5abba86107d..a209bd67209e 100644
--- a/include/linux/fs.h
+++ b/include/linux/fs.h
@@ -950,6 +950,8 @@ struct file {
 	struct inode		*f_inode;	/* cached value */
 	const struct file_operations	*f_op;
 
+	int _bpf_level;
+
 	/*
 	 * Protects f_ep_links, f_flags.
 	 * Must not be taken from IRQ context.
diff --git a/include/linux/syscalls.h b/include/linux/syscalls.h
index b951a87da987..84e9b80c4bf3 100644
--- a/include/linux/syscalls.h
+++ b/include/linux/syscalls.h
@@ -1424,4 +1424,6 @@ long compat_ksys_semtimedop(int semid, struct sembuf __user *tsems,
 			    unsigned int nsops,
 			    const struct old_timespec32 __user *timeout);
 
+asmlinkage long sys_set_bpf_level(int fd, int level);
+
 #endif
diff --git a/include/uapi/linux/bpf.h b/include/uapi/linux/bpf.h
index 8bd33050b7bb..81f5bf3768fd 100644
--- a/include/uapi/linux/bpf.h
+++ b/include/uapi/linux/bpf.h
@@ -189,6 +189,7 @@ enum bpf_prog_type {
 	BPF_PROG_TYPE_STRUCT_OPS,
 	BPF_PROG_TYPE_EXT,
 	BPF_PROG_TYPE_LSM,
+	BPF_PROG_TYPE_STORAGE,
 };
 
 enum bpf_attach_type {
@@ -226,7 +227,8 @@ enum bpf_attach_type {
 	BPF_CGROUP_INET4_GETSOCKNAME,
 	BPF_CGROUP_INET6_GETSOCKNAME,
 	BPF_XDP_DEVMAP,
-	__MAX_BPF_ATTACH_TYPE
+	BPF_STORAGE,
+	__MAX_BPF_ATTACH_TYPE,
 };
 
 #define MAX_BPF_ATTACH_TYPE __MAX_BPF_ATTACH_TYPE
@@ -4261,4 +4263,9 @@ struct bpf_pidns_info {
 	__u32 pid;
 	__u32 tgid;
 };
+
+struct bpf_storage {
+	char data[512];
+};
+
 #endif /* _UAPI__LINUX_BPF_H__ */
diff --git a/kernel/bpf/syscall.c b/kernel/bpf/syscall.c
index 0fd80ac81f70..8550179bbc43 100644
--- a/kernel/bpf/syscall.c
+++ b/kernel/bpf/syscall.c
@@ -2815,6 +2815,8 @@ attach_type_to_prog_type(enum bpf_attach_type attach_type)
 		return BPF_PROG_TYPE_CGROUP_SOCKOPT;
 	case BPF_TRACE_ITER:
 		return BPF_PROG_TYPE_TRACING;
+	case BPF_STORAGE:
+		return BPF_PROG_TYPE_STORAGE;
 	default:
 		return BPF_PROG_TYPE_UNSPEC;
 	}
@@ -2825,6 +2827,9 @@ attach_type_to_prog_type(enum bpf_attach_type attach_type)
 #define BPF_F_ATTACH_MASK \
 	(BPF_F_ALLOW_OVERRIDE | BPF_F_ALLOW_MULTI | BPF_F_REPLACE)
 
+int _storage_bpf_prog_attach(const union bpf_attr *attr, struct bpf_prog *prog);
+int _storage_bpf_prog_detach(const union bpf_attr *attr);
+
 static int bpf_prog_attach(const union bpf_attr *attr)
 {
 	enum bpf_prog_type ptype;
@@ -2870,6 +2875,9 @@ static int bpf_prog_attach(const union bpf_attr *attr)
 	case BPF_PROG_TYPE_SOCK_OPS:
 		ret = cgroup_bpf_prog_attach(attr, ptype, prog);
 		break;
+	case BPF_PROG_TYPE_STORAGE:
+		ret = _storage_bpf_prog_attach(attr, prog);
+		break;
 	default:
 		ret = -EINVAL;
 	}
@@ -2906,6 +2914,8 @@ static int bpf_prog_detach(const union bpf_attr *attr)
 	case BPF_PROG_TYPE_CGROUP_SYSCTL:
 	case BPF_PROG_TYPE_SOCK_OPS:
 		return cgroup_bpf_prog_detach(attr, ptype);
+	case BPF_PROG_TYPE_STORAGE:
+		return _storage_bpf_prog_detach(attr);
 	default:
 		return -EINVAL;
 	}
